{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3 - Generalization, Overfitting, and Underfitting.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPJTRR68AC8wpdL4tYCweVU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chaurasiauttkarsh/Machine-Learning/blob/master/3_Generalization%2C_Overfitting%2C_and_Underfitting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OyarUI4eEbOJ",
        "colab_type": "text"
      },
      "source": [
        "**Generalization, Overfitting, and Underfitting**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0aYtdFwEdmm",
        "colab_type": "text"
      },
      "source": [
        "In supervised learning, we want to build a model on the training data and then be able to make accurate predictions on new, unseen data that has the same characteris‚Äê tics as the training set that we used. If a model is able to make accurate predictions on unseen data, we say it is able to generalize from the training set to the test set. We want to build a model that is able to generalize as accurately as possible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "taOALGvAEhVR",
        "colab_type": "text"
      },
      "source": [
        "Usually we build a model in such a way that it can make accurate predictions on the training set. If the training and test sets have enough in common, we expect the model to also be accurate on the test set. However, there are some cases where this can go wrong. For example, if we allow ourselves to build very complex models, we can always be as accurate as we like on the training set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jd0jHkIAExTp",
        "colab_type": "text"
      },
      "source": [
        "Building a model that is too complex for the amount of information we have is called overfitting. Overfitting occurs when you fit a model too closely to the particularities of the training set and obtain a model that works well on the training set but is not able to generalize to new data. On the other hand, if your model is too simple your model will do badly even on the training set. Choosing too simple a model is called underfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0D4aAHxE_oj",
        "colab_type": "text"
      },
      "source": [
        "![Sweet Spot Between Underfitting and Overfitting](https://drive.google.com/uc?id=1nvb1gQukfln91ExAke-zQyRwTmxLStJu)"
      ]
    }
  ]
}
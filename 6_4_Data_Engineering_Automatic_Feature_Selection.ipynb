{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "6.4 - Data Engineering - Automatic Feature Selection.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPdK6Yv57Wdtl63YXKS4VGz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chaurasiauttkarsh/Machine-Learning/blob/master/6_4_Data_Engineering_Automatic_Feature_Selection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8UdFRHSHvYo",
        "colab_type": "text"
      },
      "source": [
        "With so many ways to create new features, you might get tempted to increase the dimensionality of the data way beyond the number of original features. However, adding more features makes all models more complex, and so increases the chance of overfitting. When adding new features, or with high-dimensional datasets in general, it can be a good idea to reduce the number of features to only the most useful ones, and discard the rest. This can lead to simpler models that generalize better. But how can you know how good each feature is? There are three basic strategies: univariate statistics, model-based selection, and iterative selection. We will discuss all three of them in detail. All of these methods are supervised methods, meaning they need the target for fitting the model. This means we need to split the data into training and test sets, and fit the feature selection only on the training part of the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Azi5EKoxH9mw",
        "colab_type": "text"
      },
      "source": [
        "**Univariate Statistics**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dm7-waP2IAyZ",
        "colab_type": "text"
      },
      "source": [
        "In univariate statistics, we compute whether there is a statistically significant relationship between each feature and the target. In the case of classification, this is also known as analysis of variance (ANOVA). A key property of these tests is that they are univariate, meaning that they only consider each feature individually. Consequently, a feature will be discarded if it is only informative when combined with another feature. Univariate tests are often very fast to compute, and don’t require building a model. On the other hand, they are completely independent of the model that you might want to apply after the feature selection."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQaUYvS9KY9r",
        "colab_type": "text"
      },
      "source": [
        "To use univariate feature selection in scikit-learn, you need to choose a test, usually either f_classif (the default) for classification or f_regression for regression, and a method to discard features based on the p-values determined in the test. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7jBcwi3K1cE",
        "colab_type": "text"
      },
      "source": [
        "All methods for discarding parameters use a threshold to discard all features with too high a p-value (which means they are unlikely to be related to the target). The methods differ in how they compute this threshold, with the simplest ones being SelectKBest, which selects a fixed number k of features, and SelectPercentile, which selects a fixed percentage of features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gCy_dD6K_q7",
        "colab_type": "text"
      },
      "source": [
        "Let’s apply the feature selection for classification on the cancer dataset. To make the task a bit harder, we’ll add some noninformative noise features to the data. We expect the feature selection to be able to identify the features that are noninformative and remove them:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ag3oTxwFJm1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "d2c08db4-92c3-443e-b9f9-f2b0bbeb4c6b"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.feature_selection import SelectPercentile \n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "cancer = load_breast_cancer()\n",
        "\n",
        "# get deterministic random numbers\n",
        "rng = np.random.RandomState(42)\n",
        "noise = rng.normal(size=(len(cancer.data), 50))\n",
        "# add noise features to the data\n",
        "# the first 30 features are from the dataset, the next 50 are noise \n",
        "X_w_noise = np.hstack([cancer.data, noise])\n",
        "X_train, X_test, y_train, y_test = train_test_split( X_w_noise, cancer.target, random_state=0, test_size=.5)\n",
        "# use f_classif (the default) and SelectPercentile to select 50% of features\n",
        "select = SelectPercentile(percentile=50) \n",
        "select.fit(X_train, y_train)\n",
        "# transform training set\n",
        "X_train_selected = select.transform(X_train)\n",
        "print(\"X_train.shape: {}\".format(X_train.shape)) \n",
        "print(\"X_train_selected.shape: {}\".format(X_train_selected.shape))\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train.shape: (284, 80)\n",
            "X_train_selected.shape: (284, 40)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PA1aU8_PL3-g",
        "colab_type": "text"
      },
      "source": [
        "As you can see, the number of features was reduced from 80 to 40 (50 percent of the original number of features). We can find out which features have been selected using the get_support method, which returns a Boolean mask of the selected features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "waTavLIaL6sp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "outputId": "4dbae066-7293-4e73-e11a-6ce1f98773de"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "mask = select.get_support()\n",
        "print(mask)\n",
        "# visualize the mask -- black is True, white is False \n",
        "plt.matshow(mask.reshape(1, -1), cmap='gray_r') \n",
        "plt.xlabel(\"Sample index\")\n",
        "plt.yticks(())"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ True  True  True  True  True  True  True  True  True False  True False\n",
            "  True  True  True  True  True  True False False  True  True  True  True\n",
            "  True  True  True  True  True  True False False False  True False  True\n",
            " False False  True False False False False  True False False  True False\n",
            " False  True False  True False False False False False False  True False\n",
            "  True False False False False  True False  True False False False False\n",
            "  True  True False  True False False False False]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([], <a list of 0 Text major ticklabel objects>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA44AAAA4CAYAAACyjKZhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAJw0lEQVR4nO3dW6xcVR3H8e/PAqJgREo1Ri6VgCASrbQqBAJYlIAS9YGoqAkaEx7kQaPEeMELRB/wwUtUUASUBEQFQYkmBoI03rE9iLaAKJIiKNA2inhJiuDfh1mFk2MZzmWmZ1b9fpJm9l57z6x15jd7z/mffWmqCkmSJEmSnshTFnsAkiRJkqTJZuEoSZIkSRrKwlGSJEmSNJSFoyRJkiRpKAtHSZIkSdJQFo6SJEmSpKEWVDgmOSnJHUnuTPKBUQ1K45HkkiSbkmyY1rZ3kuuT/L49Pmsxx6jtS7JfkhuT3Jbk1iTvbu3m14Ekuyf5ZZJft/zOae3PT3JT24d+M8luiz1WbV+SJUl+leR7bd7sOpFkY5L1SW5Jsq61ue/sQJK9klyV5LdJbk9ylNn1IckhbZvb9u+hJO8xv77Nu3BMsgT4InAycBhwWpLDRjUwjcXXgJNmtH0AuKGqDgZuaPOaPI8A76uqw4AjgTPb9mZ+fdgKrK6qlwArgJOSHAmcB3ymqg4C/gq8cxHHqOHeDdw+bd7s+vLKqlpRVavavPvOPnwO+EFVHQq8hME2aHYdqKo72ja3AlgJ/Au4BvPr2kKOOL4cuLOq7qqqh4FvAK8fzbA0DlX1I+AvM5pfD1zapi8F3rBDB6VZqar7qurmNv13Bl+ez8P8ulAD/2izu7Z/BawGrmrt5jehkuwLvBa4qM0Hs+ud+84Jl+SZwLHAxQBV9XBVPYjZ9egE4A9VdTfm17WFFI7PA+6ZNn9va1NfnlNV97Xp+4HnLOZg9OSSLAdeCtyE+XWjnep4C7AJuB74A/BgVT3SVnEfOrk+C7wf+E+bX4rZ9aSA65JMJTmjtbnvnHzPBzYDX22niV+UZA/MrkdvBq5o0+bXMW+Oo8dUVTH4gtWESrIn8G3gPVX10PRl5jfZqurRdsrOvgzO2Dh0kYekWUhyCrCpqqYWeyyat2Oq6ggGl9acmeTY6Qvdd06sXYAjgAuq6qXAP5lxWqPZTb52/ffrgCtnLjO//iykcPwTsN+0+X1bm/ryQJLnArTHTYs8Hj2BJLsyKBovr6qrW7P5daadanUjcBSwV5Jd2iL3oZPpaOB1STYyuCRjNYPrrsyuE1X1p/a4icE1Vi/HfWcP7gXuraqb2vxVDApJs+vLycDNVfVAmze/ji2kcFwLHNzuLLcbg8PQ145mWNqBrgVOb9OnA99dxLHoCbRrqi4Gbq+qT09bZH4dSLIsyV5t+mnAqxlcp3ojcGpbzfwmUFV9sKr2rarlDL7nflhVb8XsupBkjyTP2DYNnAhswH3nxKuq+4F7khzSmk4AbsPsenMaj5+mCubXtQyOEs/zyclrGFz7sQS4pKo+OaqBafSSXAEcD+wDPAB8DPgO8C1gf+Bu4I1VNfMGOlpkSY4Bfgys5/HrrD7E4DpH85twSV7M4CYASxj8we5bVXVukgMZHMXaG/gV8Laq2rp4I9UwSY4HzqqqU8yuDy2na9rsLsDXq+qTSZbivnPiJVnB4KZUuwF3Ae+g7UMxu4nX/ljzR+DAqvpba3Pb69iCCkdJkiRJ0s7Pm+NIkiRJkoaycJQkSZIkDWXhKEmSJEkaysJRkiRJkjSUhaMkSZIkaagFF45JzhjFQLQ4zK9fZtc38+ub+fXL7Ppmfv0yu/6N4oijH4K+mV+/zK5v5tc38+uX2fXN/Ppldp3zVFVJkiRJ0lCpqtmvnMx+ZY3dypUr57T+1NTUWF57Lq+7s5trJrPle9y37X0uNm/ezLJly3bYGOb6GRrXPmAStpFRjGGS85uE93hSxjGXbW8S3uMe7ez7lnGahN/LduT7Nqr95rh+J9oZ3uNRmZqa2lJV/xOWhWPH5pIdQJKxvPZcXndnN9dMZsv3uG/j+lzMxVw/Q+PaB0zCNjIJeczVJPx84/wMjWscvX2Oe7Sz71vGyc/y/IzrdyLf48clmaqqVTPbPVVVkiRJkjSUhaMkSZIkaSgLR0mSJEnSUBaOkiRJkqShLBwlSZIkSUNZOEqSJEmShrJwlCRJkiQNZeEoSZIkSRrKwlGSJEmSNFSqavYrJ5uBu2c07wNsGeWgtEOZX7/Mrm/m1zfz65fZ9c38+mV2/TigqpbNbJxT4bg9SdZV1aoFvYgWjfn1y+z6Zn59M79+mV3fzK9fZtc/T1WVJEmSJA1l4ShJkiRJGmoUheOFI3gNLR7z65fZ9c38nkSSDye5NclvktyS5BVj7m9NktmeRnVhknOTvGqOfWxMss88hqfRcdvrm/n1y+w6t+BrHCVJGrUkRwGfBo6vqq2t2Nqtqv48xj7XAGdV1box9rERWFVV3iBCktQVT1WVJE2i5wJbqmorQFVt2VY0JvlokrVJNiS5MEla+5okn0myLsntSV6W5Ookv0/yibbO8iS/TXJ5W+eqJE+f2XmSE5P8PMnNSa5Msud21vlaklPb9MYk57T11yc5tLUvTXJdO3J6EZBpz39bkl+2o6lfTrKkjfk3SXZPskd73uGjf3slSZobC0dJ0iS6Dtgvye+SnJ/kuGnLvlBVL6uqw4GnAadMW/Zwu2vfl4DvAmcChwNvT7K0rXMIcH5VvRB4CHjX9I7b0c2zgVdV1RHAOuC9sxjzlrb+BcBZre1jwE+q6kXANcD+rY8XAm8Cjq6qFcCjwFurai1wLfAJ4FPAZVW1YRZ9S5I0VhaOkqSJU1X/AFYCZwCbgW8meXtb/MokNyVZD6wGXjTtqde2x/XArVV1XztqeRewX1t2T1X9tE1fBhwzo/sjgcOAnya5BTgdOGAWw766PU4By9v0sa0Pqur7wF9b+wnt51vb+jgBOLAtOxd4NbCKQfEoSdKi22WxByBJ0vZU1aPAGmBNKxJPT/IN4HwG1wnek+TjwO7Tnra1Pf5n2vS2+W3feTMv7p85H+D6qjptjkPe1t+jPPn3a4BLq+qD21m2FNgT2JXBz/bPOY5DkqSR84ijJGniJDkkycHTmlYAd/N4kbilXXd46jxefv928x2AtwA/mbH8F8DRSQ5qY9kjyQvm0Q/Aj1ofJDkZeFZrvwE4Ncmz27K9k2w7qvll4CPA5cB58+xXkqSR8oijJGkS7Ql8PslewCPAncAZVfVgkq8AG4D7gbXzeO07gDOTXALcxuCaxMdU1eZ2WuwVSZ7ams8GfjePvs5pr3Mr8DPgj62P25KcDVyX5CnAv9uYjgP+XVVfT7IE+FmS1VX1w3n0LUnSyPjfcUiS/m8kWQ58r91YR5IkzZKnqkqSJEmShvKIoyRJkiRpKI84SpIkSZKGsnCUJEmSJA1l4ShJkiRJGsrCUZIkSZI0lIWjJEmSJGkoC0dJkiRJ0lD/BUzgiIuuLq3KAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 1152x144 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45FIn_sCMPrT",
        "colab_type": "text"
      },
      "source": [
        "As you can see from the visualization of the mask, most of the selected features are the original features, and most of the noise features were removed. However, the recovery of the original features is not perfect. Let’s compare the performance of logistic regression on all features against the performance using only the selected features:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKWoEBu1MSNU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "47fb1a18-610b-4637-812b-3b9a6a138070"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression \n",
        "# transform test data\n",
        "X_test_selected = select.transform(X_test)\n",
        "lr = LogisticRegression()\n",
        "lr.fit(X_train, y_train)\n",
        "print(\"Score with all features: {:.3f}\".format(lr.score(X_test, y_test))) \n",
        "lr.fit(X_train_selected, y_train)\n",
        "print(\"Score with only selected features: {:.3f}\".format(lr.score(X_test_selected, y_test)))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Score with all features: 0.916\n",
            "Score with only selected features: 0.919\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Dx_NCaQMheU",
        "colab_type": "text"
      },
      "source": [
        "Univariate feature selection can still be very helpful, though, if there is such a large number of features that building a model on them is infeasible, or if you suspect that many features are completely uninformative."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVDEPLMDMk1S",
        "colab_type": "text"
      },
      "source": [
        "**Model Based Feature selection**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UoOuCl3gOg2b",
        "colab_type": "text"
      },
      "source": [
        "Model-based feature selection uses a supervised machine learning model to judge the importance of each feature, and keeps only the most important ones. The supervised model that is used for feature selection doesn’t need to be the same model that is used for the final supervised modeling. The feature selection model needs to provide some measure of importance for each feature, so that they can be ranked by this measure. Decision trees and decision tree–based models provide a feature_importance_ attribute, which directly encodes the importance of each feature. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8eLzfhXQE4T",
        "colab_type": "text"
      },
      "source": [
        "Linear models have coefficients, which can also be used to capture feature importances by considering the absolute values. As we saw in linear models with L1 penalty learn sparse coefficients, which only use a small subset of features. This can be viewed as a form of feature selection for the model itself, but can also be used as a preprocessing step to select features for another model. In contrast to univariate selection, model-based selection considers all features at once, and so can capture interactions (if the model can capture them). To use model-based feature selection, we need to use the SelectFromModel transformer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXZmVEQSQNkL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_selection import SelectFromModel \n",
        "from sklearn.ensemble import RandomForestClassifier \n",
        "select = SelectFromModel(RandomForestClassifier(n_estimators=100, random_state=42), threshold=\"median\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFdsML04Qa6S",
        "colab_type": "text"
      },
      "source": [
        "The SelectFromModel class selects all features that have an importance measure of the feature (as provided by the supervised model) greater than the provided threshold. To get a comparable result to what we got with univariate feature selection, we used the median as a threshold, so that half of the features will be selected. We use a random forest classifier with 100 trees to compute the feature importances. This is a quite complex model and much more powerful than using univariate tests. Now let’s actually fit the model:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9cjkihjQhgL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "2db35230-cffb-4819-e961-ac50eb3082fe"
      },
      "source": [
        "select.fit(X_train, y_train)\n",
        "X_train_l1 = select.transform(X_train) \n",
        "print(\"X_train.shape: {}\".format(X_train.shape)) \n",
        "print(\"X_train_l1.shape: {}\".format(X_train_l1.shape))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train.shape: (284, 80)\n",
            "X_train_l1.shape: (284, 40)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDGlZ98uQo4d",
        "colab_type": "text"
      },
      "source": [
        "Again, we can have a look at the features that were selected."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-IJRQq1Qq9R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "56c1112d-7083-4a3d-e91a-bfddcc54a0b6"
      },
      "source": [
        "mask = select.get_support()\n",
        "# visualize the mask -- black is True, white is False \n",
        "plt.matshow(mask.reshape(1, -1), cmap='gray_r') \n",
        "plt.xlabel(\"Sample index\")\n",
        "plt.yticks(())"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([], <a list of 0 Text major ticklabel objects>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1152x144 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S44L-kIRQ2i7",
        "colab_type": "text"
      },
      "source": [
        "This time, all but two of the original features were selected. Because we specified to select 40 features, some of the noise features are also selected. Let’s take a look at the performance:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5bDL3MuQ4Wv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "86cea501-5895-42e4-c1ff-35bca87b47f1"
      },
      "source": [
        "X_test_l1 = select.transform(X_test)\n",
        "score = LogisticRegression().fit(X_train_l1, y_train).score(X_test_l1, y_test) \n",
        "print(\"Test score: {:.3f}\".format(score))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test score: 0.930\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNYebbkoQ9bO",
        "colab_type": "text"
      },
      "source": [
        "**Iterative Feature Selection**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3xe_5nlRBvg",
        "colab_type": "text"
      },
      "source": [
        "In iterative feature selection, a series of models are built, with varying numbers of features. There are two basic methods: starting with no features and adding features one by one until some stopping criterion is reached, or starting with all features and removing features one by one until some stopping criterion is reached. Because a series of models are built, these methods are much more computationally expensive than the methods we discussed previously. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5O2h0A3Rs7A",
        "colab_type": "text"
      },
      "source": [
        "One particular method of this kind is recursive feature elimination (RFE), which starts with all features, builds a model, and discards the least important feature according to the model. Then a new model is built using all but the discarded feature, and so on until only a prespecified number of features are left. For this to work, the model used for selection needs to provide some way to determine feature importance, as was the case for the model-based selection. Here, we use the same random forest model that we used earlier,"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChMzNhpRSKv6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "cee95b6a-9d6a-46a2-85af-168db06abcd4"
      },
      "source": [
        "from sklearn.feature_selection import RFE\n",
        "select = RFE(RandomForestClassifier(n_estimators=100, random_state=42),n_features_to_select=40)\n",
        "select.fit(X_train, y_train)\n",
        "# visualize the selected features:\n",
        "mask = select.get_support() \n",
        "plt.matshow(mask.reshape(1, -1), cmap='gray_r') \n",
        "plt.xlabel(\"Sample index\")\n",
        "plt.yticks(())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([], <a list of 0 Text major ticklabel objects>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1152x144 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlUimsQ7SZ3g",
        "colab_type": "text"
      },
      "source": [
        "The feature selection got better compared to the univariate and model-based selection, but one feature was still missed. Running this code also takes significantly longer than that for the model-based selection, because a random forest model is trained 40 times, once for each feature that is dropped. Let’s test the accuracy of the logistic regression model when using RFE for feature selection:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "boB5x5bbU2_i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "ce826fbe-08d6-4b75-fe76-9e8fec1e2091"
      },
      "source": [
        "X_train_rfe = select.transform(X_train) \n",
        "X_test_rfe = select.transform(X_test)\n",
        "score = LogisticRegression().fit(X_train_rfe, y_train).score(X_test_rfe, y_test) \n",
        "print(\"Test score: {:.3f}\".format(score))\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test score: 0.930\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3zsi7aEU2Y3",
        "colab_type": "text"
      },
      "source": [
        "We can also use the model used inside the RFE to make predictions. This uses only the feature set that was selected:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ohexh3RiVBKp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "181429c3-01df-46a4-d2d2-85a2ae8f4a2f"
      },
      "source": [
        "print(\"Test score: {:.3f}\".format(select.score(X_test, y_test)))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test score: 0.951\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wde4WHfQVNNm",
        "colab_type": "text"
      },
      "source": [
        "If you are unsure when selecting what to use as input to your machine learning algo‐ rithms, automatic feature selection can be quite helpful. It is also great for reducing the amount of features needed—for example, to speed up prediction or to allow for more interpretable models. In most real-world cases, applying feature selection is unlikely to provide large gains in performance. However, it is still a valuable tool in the toolbox of the feature engineer."
      ]
    }
  ]
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/chaurasiauttkarsh/Machine-Learning/blob/master/4_2_Supervised_Learning_Linear_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7X3JuES_Gt8R"
   },
   "source": [
    "**Linear Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XY43Iaf-HEni"
   },
   "source": [
    "**Linear Models for Regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XYZYuvkTHJIL"
   },
   "source": [
    "For regression, the general prediction formula for a linear model looks as follows: \n",
    "\n",
    "ŷ = w[0] * x[0] + w[1] * x[1] + ... + w[p] * x[p] + b\n",
    "\n",
    "Here, x[0] to x[p] denotes the features (in this example, the number of features is p+1) of a single data point, w and b are parameters of the model that are learned, and ŷ is the prediction the model makes. \n",
    "\n",
    "For a dataset with a single feature, this is:\n",
    "ŷ = w[0] * x[0] + b\n",
    "which you might remember from high school mathematics as the equation for a line. Here, w[0] is the slope and b is the y-axis offset. For more features, w contains the slopes along each feature axis. Alternatively, you can think of the predicted response as being a weighted sum of the input features, with weights (which can be negative) given by the entries of w."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    },
    "colab_type": "code",
    "id": "w-T1RYXFHmyi",
    "outputId": "da0bf965-5858-4750-8cc2-78ec3138c806"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mglearn in c:\\users\\dhruv aggarwal\\anaconda3\\lib\\site-packages (0.1.7)\n",
      "Requirement already satisfied: cycler in c:\\users\\dhruv aggarwal\\anaconda3\\lib\\site-packages (from mglearn) (0.10.0)\n",
      "Requirement already satisfied: imageio in c:\\users\\dhruv aggarwal\\anaconda3\\lib\\site-packages (from mglearn) (2.6.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\dhruv aggarwal\\anaconda3\\lib\\site-packages (from mglearn) (0.25.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\dhruv aggarwal\\anaconda3\\lib\\site-packages (from mglearn) (3.1.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\dhruv aggarwal\\anaconda3\\lib\\site-packages (from mglearn) (1.16.5)\n",
      "Requirement already satisfied: pillow in c:\\users\\dhruv aggarwal\\anaconda3\\lib\\site-packages (from mglearn) (6.2.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\dhruv aggarwal\\anaconda3\\lib\\site-packages (from mglearn) (0.21.3)\n",
      "Requirement already satisfied: six in c:\\users\\dhruv aggarwal\\anaconda3\\lib\\site-packages (from cycler->mglearn) (1.12.0)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\users\\dhruv aggarwal\\anaconda3\\lib\\site-packages (from pandas->mglearn) (2.8.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\dhruv aggarwal\\anaconda3\\lib\\site-packages (from pandas->mglearn) (2019.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\dhruv aggarwal\\anaconda3\\lib\\site-packages (from matplotlib->mglearn) (1.1.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\users\\dhruv aggarwal\\anaconda3\\lib\\site-packages (from matplotlib->mglearn) (2.4.2)\n",
      "Requirement already satisfied: scipy>=0.17.0 in c:\\users\\dhruv aggarwal\\anaconda3\\lib\\site-packages (from scikit-learn->mglearn) (1.3.1)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\dhruv aggarwal\\anaconda3\\lib\\site-packages (from scikit-learn->mglearn) (0.13.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\dhruv aggarwal\\anaconda3\\lib\\site-packages (from kiwisolver>=1.0.1->matplotlib->mglearn) (41.4.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install mglearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 552
    },
    "colab_type": "code",
    "id": "gkDDOML2Hlkz",
    "outputId": "6eee80fd-0b92-40e0-ee0d-d05bb416601c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w[0]: 0.393906  b: -0.031804\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import mglearn\n",
    "mglearn.plots.plot_linear_regression_wave()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "It9Zbd3PHjLS"
   },
   "source": [
    "Looking at w[0] we see that the slope should be around 0.4, which we can confirm visually in the plot. The intercept is where the prediction line should cross the y-axis: this is slightly below zero, which you can also confirm in the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zw2_K1RVIBuu"
   },
   "source": [
    "Linear models for regression can be characterized as regression models for which the prediction is a line for a single feature, a plane when using two features, or a hyper‐ plane in higher dimensions (that is, when using more features)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "19d0cRQOIF4l"
   },
   "source": [
    "If you compare the predictions made by the straight line with those made by the KNeighborsRegressor, using a straight line to make predictions seems very restrictive. It looks like all the fine details of the data are lost. In a sense, this is true. It is a strong (and somewhat unrealistic) assumption that our target y is a linear combination of the features. But looking at one-dimensional data gives a somewhat skewed perspective. For datasets with many features, linear models can be very powerful. In particular, if you have more features than training data points, any target y can be perfectly modeled (on the training set) as a linear function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pgBCH9ZUITxl"
   },
   "source": [
    "There are many different linear models for regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DSbpC583IYtq"
   },
   "source": [
    "**Linear regression (aka ordinary least squares)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KtO_ozB-Iiod"
   },
   "source": [
    "Linear regression, or ordinary least squares (OLS), is the simplest and most classic linear method for regression. Linear regression finds the parameters w and b that minimize the mean squared error between predictions and the true regression targets, y, on the training set. The mean squared error is the sum of the squared differences between the predictions and the true values, divided by the number of samples. Linear regression has no parameters, which is a benefit, but it also has no way to control model complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ppGev5GGAA0V"
   },
   "source": [
    "**Step 1 - Loading the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eabgGp-ZAGAe"
   },
   "outputs": [],
   "source": [
    "import mglearn\n",
    "X, y = mglearn.datasets.make_wave(n_samples=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cg-REQbhARYo"
   },
   "source": [
    "**Step 2 - Train-Test Split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mq-foOmYAe0R"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XsHsn3gQAzHU"
   },
   "source": [
    "**Step 3 - Building the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AkCQzWW2A9gr"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1jUdyKbDBRPM"
   },
   "source": [
    "**Step 4 - Training the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "5S_guMsFBZhe",
    "outputId": "36680d12-5da5-42ef-aad4-2614fdf3be46"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qoZKoLMhBeX-"
   },
   "source": [
    "**Step 5 - Making the Predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "7CbTF4D_Bmdg",
    "outputId": "e3e6fefc-1ba8-44a2-ac07-151af61caf6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set prediction [ 0.90558131 -0.90145215  0.42515939 -0.57820859 -0.85432225 -0.61256977\n",
      " -0.02372291  0.69169983 -0.9696576   0.48810788]\n"
     ]
    }
   ],
   "source": [
    "print(\"Test set prediction {}\".format(lr.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PBqk0T4tByBd"
   },
   "source": [
    "**Step 6 - Evaluating the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "A74k1yVXB12s",
    "outputId": "1de47e9c-4cff-4e8f-822b-4d1abb9d8afb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score : 0.6835587020533835\n",
      "Test score : 0.6735973782183866\n"
     ]
    }
   ],
   "source": [
    "print(\"Train score : {}\".format(lr.score(X_train, y_train)))\n",
    "print(\"Test score : {}\".format(lr.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hC5EHnCf3IZm"
   },
   "source": [
    "The “slope” parameters (w), also called weights or coefficients, are stored in the coef_ attribute, while the offset or intercept (b) is stored in the intercept_ attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "NgSl-mMKGjkj",
    "outputId": "4b87600b-eabf-4846-892c-3dc14a6a1c09"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr.coef_: [0.44008435]\n",
      "lr.intercept_: -0.06130903623121073\n"
     ]
    }
   ],
   "source": [
    "print(\"lr.coef_: {}\".format(lr.coef_)) \n",
    "print(\"lr.intercept_: {}\".format(lr.intercept_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KvJnSGIl3SQB"
   },
   "source": [
    "The intercept_ attribute is always a single float number, while the coef_ attribute is a NumPy array with one entry per input feature. As we only have a single input feature in the wave dataset, lr.coef_ only has a single entry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ei3Rffvr3ep4"
   },
   "source": [
    "An R2 of around 0.66 is not very good, but we can see that the scores on the training and test sets are very close together. This means we are likely underfitting, not over‐ fitting. For this one-dimensional dataset, there is little danger of overfitting, as the model is very simple (or restricted). However, with higher-dimensional datasets (meaning datasets with a large number of features), linear models become more pow‐ erful, and there is a higher chance of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NJAZUzKa3xaS"
   },
   "source": [
    "Let’s take a look at how LinearRe gression performs on a more complex dataset, like the Boston Housing dataset. Remember that this dataset has 506 samples and 105 derived features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hsddi7lL33AB"
   },
   "source": [
    "**Step 1 - Loading the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e_o7_1q233AC"
   },
   "outputs": [],
   "source": [
    "import mglearn\n",
    "X, y = mglearn.datasets.load_extended_boston()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kuuOhl8J33AG"
   },
   "source": [
    "**Step 2 - Train-Test Split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VQRQj-W_33AG"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_2Oa6O4h33AI"
   },
   "source": [
    "**Step 3 - Building the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PaBWS9o033AJ"
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "lr = KNeighborsRegressor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Si0XD9iK33AM"
   },
   "source": [
    "**Step 4 - Training the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "bh4HiKJL33AM",
    "outputId": "c65866cb-7172-4ba3-c821-2167def7d536"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                    metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
       "                    weights='uniform')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k37SzsGz33AQ"
   },
   "source": [
    "**Step 5 - Making the Predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "id": "QAaW4Wv033AQ",
    "outputId": "fc0d6985-3de4-43c0-99f4-20c913e88d9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set prediction [28.28 25.26 19.06 21.34 24.24 20.16 13.96 26.38 10.04 23.64 17.94 26.04\n",
      " 21.46 26.72 21.62 21.52 21.74 19.76 27.04 19.5  14.76 12.52 25.98 29.38\n",
      " 14.82 22.9  26.04 15.06 10.38 26.38 14.42 12.52 20.82 10.38 16.82 10.24\n",
      " 27.54 13.28 12.3  19.76 20.84 26.74 21.52 16.76 24.58 11.34 24.94 23.44\n",
      " 29.   19.54 28.84 32.24 23.02 27.9  21.76 25.3  23.   33.56 22.58 21.9\n",
      " 19.5  27.24 16.3  34.24 20.38 20.82 21.66 41.74 17.94 35.52 19.5  25.16\n",
      " 20.82 23.84 16.76 14.54 32.02 12.82 21.38 40.92  8.28 14.82 34.08 15.32\n",
      " 11.02 21.9  21.16 23.88 21.7  22.66 24.1  20.74 27.24 35.46 27.04 19.06\n",
      " 18.94 21.32 35.46 27.48 11.32 19.58 20.28 25.44 32.44 26.04  9.2  14.4\n",
      " 20.6  23.62 17.94 22.5  20.42 11.84 18.92 22.24 17.94 21.28 16.42 21.52\n",
      " 14.88 42.88 20.4  10.84 30.84 10.7  21.06]\n"
     ]
    }
   ],
   "source": [
    "print(\"Test set prediction {}\".format(lr.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lwGlS3wu33AS"
   },
   "source": [
    "**Step 6 - Evaluating the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "TSdpEma433AT",
    "outputId": "232730ba-94ed-4c48-ade7-28c4234ea6b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score : 0.8235989534796126\n",
      "Test score : 0.7181964594261468\n"
     ]
    }
   ],
   "source": [
    "print(\"Train score : {}\".format(lr.score(X_train, y_train)))\n",
    "print(\"Test score : {}\".format(lr.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "colab_type": "code",
    "id": "taLjf0zA4YoO",
    "outputId": "16285fe4-dbb1-4e9f-9c95-78d31ad8f0ef"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'KNeighborsRegressor' object has no attribute 'coef_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-6cdc31f58916>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"lr.coef_: {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"lr.intercept_: {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintercept_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'KNeighborsRegressor' object has no attribute 'coef_'"
     ]
    }
   ],
   "source": [
    "print(\"lr.coef_: {}\".format(lr.coef_)) \n",
    "print(\"lr.intercept_: {}\".format(lr.intercept_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jOIepP3w4eP0"
   },
   "source": [
    "This discrepancy between performance on the training set and the test set is a clear sign of overfitting, and therefore we should try to find a model that allows us to control complexity. One of the most commonly used alternatives to standard linear regression is ridge regression, which we will look into next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gd1eSbem4iPT"
   },
   "source": [
    "**Ridge Regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4aWLvGPM4sb2"
   },
   "source": [
    "Ridge regression is also a linear model for regression, so the formula it uses to make predictions is the same one used for ordinary least squares. In ridge regression, though, the coefficients (w) are chosen not only so that they predict well on the training data, but also to fit an additional constraint. We also want the magnitude of coefficients to be as small as possible; in other words, all entries of w should be close to zero. Intuitively, this means each feature should have as little effect on the outcome as possible (which translates to having a small slope), while still predicting well. This constraint is an example of what is called regularization. Regularization means explicitly restricting a model to avoid overfitting. The particular kind used by ridge regression is known as L2 regularization. (**Regularisations will be taught later in detail**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WyVd2yJN5LXC"
   },
   "source": [
    "**Step 1 - Loading the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2jj37h_95KYn"
   },
   "outputs": [],
   "source": [
    "X, y = mglearn.datasets.load_extended_boston()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5oJaoVsA5kT1"
   },
   "source": [
    "**Step 2 - Train-Test Split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y0YruEKX5q55"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m8NGj7N-5-jg"
   },
   "source": [
    "**Step 3 - Building the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qYKUSJUS6Gui"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "ridge = Ridge()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ohUdbave6fJe"
   },
   "source": [
    "**Step 4 - Training the model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gQSZRjws6mm6"
   },
   "source": [
    "Now, we fit the classifier using the training set. For KNeighborsClassifier this means storing the dataset, so we can compute neighbors during prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "2xnyzGFv6nPq",
    "outputId": "443552c0-9977-42e9-b63b-a5cce2d10f96"
   },
   "outputs": [],
   "source": [
    "ridge.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SVKEHoID60B9"
   },
   "source": [
    "**Step 5 - Making the predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "id": "ugyVaYoi7AlJ",
    "outputId": "f9f482c9-f043-4669-9355-ae32a290db3f"
   },
   "outputs": [],
   "source": [
    "print(\"Test set prediction {}\".format(ridge.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9hlsziIz7Nuh"
   },
   "source": [
    "**Step 6 - Evaluating the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "uVVNVmHb7SHr",
    "outputId": "895d8c11-99f9-4439-ca24-6fa147cb0d9e"
   },
   "outputs": [],
   "source": [
    "print(\"Train score : {}\".format(ridge.score(X_train, y_train)))\n",
    "print(\"Test score : {}\".format(ridge.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "colab_type": "code",
    "id": "o-khlXdL5yS1",
    "outputId": "f8c7ff57-0c3d-4732-d4d0-6173dcc1cefb"
   },
   "outputs": [],
   "source": [
    "print(\"lr.coef_: {}\".format(ridge.coef_)) \n",
    "print(\"lr.intercept_: {}\".format(ridge.intercept_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_nCsgJ9E63e6"
   },
   "source": [
    "As you can see, the training set score of Ridge is lower than for LinearRegression, while the test set score is higher. This is consistent with our expectation. With linear regression, we were overfitting our data. Ridge is a more restricted model, so we are less likely to overfit. A less complex model means worse performance on the training set, but better generalization. As we are only interested in generalization performance, we should choose the Ridge model over the LinearRegression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z1w_goJiVnlU"
   },
   "source": [
    "The Ridge model makes a trade-off between the simplicity of the model (near-zero coefficients) and its performance on the training set. How much importance the model places on simplicity versus training set performance can be specified by the user, using the alpha parameter. In the previous example, we used the default parameter alpha=1.0. There is no reason why this will give us the best trade-off, though. The optimum setting of alpha depends on the particular dataset we are using. Increasing alpha forces coefficients to move more toward zero, which decreases training set performance but might help generalization. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "hGkPaQBN67f9",
    "outputId": "1eaae568-fdf8-4c90-ba1a-a2a3a3da3500"
   },
   "outputs": [],
   "source": [
    "ridge10 = Ridge(alpha=10).fit(X_train, y_train)\n",
    "print(\"Training set score: {:.2f}\".format(ridge10.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.2f}\".format(ridge10.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CqZDrQdCVvzb"
   },
   "source": [
    "Decreasing alpha allows the coefficients to be less restricted, meaning we move right. For very small values of alpha, coefficients are barely restricted at all, and we end up with a model that resembles LinearRegression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "3Q6g27wVVzoA",
    "outputId": "ffdd55bd-7dfe-4bf9-e93c-7be5852f2a26"
   },
   "outputs": [],
   "source": [
    "ridge01 = Ridge(alpha=0.1).fit(X_train, y_train)\n",
    "print(\"Training set score: {:.2f}\".format(ridge01.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.2f}\".format(ridge01.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B6AYQHYWV-lv"
   },
   "source": [
    "We can also get a more qualitative insight into how the alpha parameter changes the model by inspecting the coef_ attribute of models with different values of alpha. A higher alpha means a more restricted model, so we expect the entries of coef_ to have smaller magnitude for a high value of alpha than for a low value of alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "colab_type": "code",
    "id": "8NQQKvLsWDOo",
    "outputId": "ba1c315e-2510-4443-fd73-829afd26763b"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(ridge.coef_, 's', label=\"Ridge alpha=1\") \n",
    "plt.plot(ridge10.coef_, '^', label=\"Ridge alpha=10\") \n",
    "plt.plot(ridge01.coef_, 'v', label=\"Ridge alpha=0.1\")\n",
    "plt.plot(lr.coef_, 'o', label=\"LinearRegression\") \n",
    "plt.xlabel(\"Coefficient index\") \n",
    "plt.ylabel(\"Coefficient magnitude\") \n",
    "plt.hlines(0, 0, len(lr.coef_))\n",
    "plt.ylim(-25, 25) \n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dg2tbfIGWrR3"
   },
   "source": [
    "Here, the x-axis enumerates the entries of coef_: x=0 shows the coefficient associated with the first feature, x=1 the coefficient associated with the second feature, and so on up to x=100. The y-axis shows the numeric values of the corresponding values of the coefficients. The main takeaway here is that for alpha=10, the coefficients are mostly between around –3 and 3. The coefficients for the Ridge model with alpha=1 are somewhat larger. The dots corresponding to alpha=0.1 have larger magnitude still, and many of the dots corresponding to linear regression without any regularization (which would be alpha=0) are so large they are outside of the chart."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "34l9CMrnWx0C"
   },
   "source": [
    "Another way to understand the influence of regularization is to fix a value of alpha but vary the amount of training data available. We subsampled the Boston Housing dataset and evaluated LinearRegression and Ridge(alpha=1) on subsets of increasing size (plots that show model performance as a function of dataset size are called learning curves):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 325
    },
    "colab_type": "code",
    "id": "pTUvdYgTWLNG",
    "outputId": "e738c970-c30d-4d34-da33-9dcd35e82a82"
   },
   "outputs": [],
   "source": [
    "mglearn.plots.plot_ridge_n_samples()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ecc00nOkXJax"
   },
   "source": [
    "As one would expect, the training score is higher than the test score for all dataset sizes, for both ridge and linear regression. Because ridge is regularized, the training score of ridge is lower than the training score for linear regression across the board. However, the test score for ridge is better, particularly for small subsets of the data. For less than 400 data points, linear regression is not able to learn anything. As more and more data becomes available to the model, both models improve, and linear regression catches up with ridge in the end. \n",
    "\n",
    "The lesson here is that with enough training data, regularization becomes less important, and given enough data, ridge and linear regression will have the same performance (the fact that this happens here when using the full dataset is just by chance). Another interesting aspect of Figure 2-13 is the decrease in training performance for linear regression. If more data is added, it becomes harder for a model to overfit, or memorize the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1uuTRNIjXVtW"
   },
   "source": [
    "**Lasso Regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Dg4nE-dWXaNP"
   },
   "source": [
    "An alternative to Ridge for regularizing linear regression is Lasso. As with ridge regression, using the lasso also restricts coefficients to be close to zero, but in a slightly different way, called L1 regularization. The consequence of L1 regularization is that when using the lasso, some coefficients are exactly zero. This means some features are entirely ignored by the model. This can be seen as a form of automatic feature selection. Having some coefficients be exactly zero often makes a model easier to interpret, and can reveal the most important features of your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wksrQVZOXzyz"
   },
   "source": [
    "**Step 1 - Loading the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tTKmfllpXzy1"
   },
   "outputs": [],
   "source": [
    "X, y = mglearn.datasets.load_extended_boston()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yBX7iY1TXzy5"
   },
   "source": [
    "**Step 2 - Train-Test Split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "teR0NsaLXzy5"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JplIWvYOXzy8"
   },
   "source": [
    "**Step 3 - Building the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3mXZx-pGXzy8"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "lasso = Lasso()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UBWat3BuXzy-"
   },
   "source": [
    "**Step 4 - Training the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "VGT2Upc1Xzy-",
    "outputId": "48ad3f97-d67a-4e66-e812-5df98fe96314"
   },
   "outputs": [],
   "source": [
    "lasso.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7LfUrf09XzzB"
   },
   "source": [
    "**Step 5 - Making the predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "id": "ObQ4WAOBXzzB",
    "outputId": "b3dc19fd-795e-4eaf-8e61-8b2ea51c1475"
   },
   "outputs": [],
   "source": [
    "print(\"Test set prediction {}\".format(lasso.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xompq0TxXzzE"
   },
   "source": [
    "**Step 6 - Evaluating the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "Z-DHUzX6XzzF",
    "outputId": "45b99a09-4f01-4200-fedc-2709492b55d5"
   },
   "outputs": [],
   "source": [
    "print(\"Train set score : {}\".format(lasso.score(X_train, y_train)))\n",
    "print(\"Test set score : {}\".format(lasso.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SxqVpI_PYvMt"
   },
   "source": [
    "As you can see, Lasso does quite badly, both on the training and the test set. This indicates that we are underfitting, and we find that it used only 4 of the 105 features. Similarly to Ridge, the Lasso also has a regularization parameter, alpha, that controls how strongly coefficients are pushed toward zero. In the previous example, we used the default of alpha=1.0. To reduce underfitting, let’s try decreasing alpha. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "nmNFtqRbY9ez",
    "outputId": "2a531320-07fc-4887-8a34-a5d44ad2df1d"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# we increase the default setting of \"max_iter\",\n",
    "# otherwise the model would warn us that we should increase max_iter. \n",
    "lasso001 = Lasso(alpha=0.01, max_iter=100000).fit(X_train, y_train) \n",
    "print(\"Training set score: {:.2f}\".format(lasso001.score(X_train, y_train))) \n",
    "print(\"Test set score: {:.2f}\".format(lasso001.score(X_test, y_test))) \n",
    "print(\"Number of features used: {}\".format(np.sum(lasso001.coef_ != 0)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fRIwEXc8ZOD5"
   },
   "source": [
    "If we set alpha too low, however, we again remove the effect of regularization and end up overfitting, with a result similar to LinearRegression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "-XrgXuRlZPHC",
    "outputId": "59bf52d0-956b-4965-be20-3a08a1935ebf"
   },
   "outputs": [],
   "source": [
    "lasso00001 = Lasso(alpha=0.0001, max_iter=100000).fit(X_train, y_train) \n",
    "print(\"Training set score: {:.2f}\".format(lasso00001.score(X_train, y_train))) \n",
    "print(\"Test set score: {:.2f}\".format(lasso00001.score(X_test, y_test))) \n",
    "print(\"Number of features used: {}\".format(np.sum(lasso00001.coef_ != 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KqXn7jsqZ1WB"
   },
   "source": [
    "Again, we can plot the coefficients of the different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "colab_type": "code",
    "id": "VYgSK3LfZ2cm",
    "outputId": "1fe0b409-4489-406f-bf71-78a62069b7a0"
   },
   "outputs": [],
   "source": [
    "plt.plot(lasso.coef_, 's', label=\"Lasso alpha=1\") \n",
    "plt.plot(lasso001.coef_, '^', label=\"Lasso alpha=0.01\") \n",
    "plt.plot(lasso00001.coef_, 'v', label=\"Lasso alpha=0.0001\")\n",
    "plt.plot(ridge01.coef_, 'o', label=\"Ridge alpha=0.1\") \n",
    "plt.legend(ncol=2, loc=(0, 1.05))\n",
    "plt.ylim(-25, 25)\n",
    "plt.xlabel(\"Coefficient index\") \n",
    "plt.ylabel(\"Coefficient magnitude\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CnwINjQ6aEgD"
   },
   "source": [
    "For alpha=1, we not only see that most of the coefficients are zero (which we already knew), but that the remaining coefficients are also small in magnitude. Decreasing alpha to 0.01, we obtain the solution shown as an upward pointing triangle, which causes most features to be exactly zero. Using alpha=0.0001, we get a model that is quite unregularized, with most coefficients nonzero and of large magnitude. For comparison, the best Ridge solution is shown as circles. The Ridge model with alpha=0.1 has similar predictive performance as the lasso model with alpha=0.01, but using Ridge, all coefficients are nonzero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ethbT_Z0afBn"
   },
   "source": [
    "In practice, ridge regression is usually the first choice between these two models. However, if you have a large amount of features and expect only a few of them to be important, Lasso might be a better choice. Similarly, if you would like to have a model that is easy to interpret, Lasso will provide a model that is easier to under‐ stand, as it will select only a subset of the input features. scikit-learn also provides the ElasticNet class, which combines the penalties of Lasso and Ridge. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h3NRcy_6afxq"
   },
   "source": [
    "**Linear Models for classification**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G1MBY0E9cihB"
   },
   "source": [
    "Linear models are also extensively used for classification. Let’s look at binary classification first. In this case, a prediction is made using the following formula:\n",
    "\n",
    "ŷ = w[0] * x[0] + w[1] * x[1] + ... + w[p] * x[p] + b > 0\n",
    "\n",
    "The formula looks very similar to the one for linear regression, but instead of just returning the weighted sum of the features, we threshold the predicted value at zero. If the function is smaller than zero, we predict the class –1; if it is larger than zero, we predict the class +1. This prediction rule is common to all linear models for classification. Again, there are many different ways to find the coefficients (w) and the intercept (b)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4tVe4VqTcxCc"
   },
   "source": [
    "For linear models for regression, the output, ŷ, is a linear function of the features: a line, plane, or hyperplane (in higher dimensions). For linear models for classification, the decision boundary is a linear function of the input. In other words, a (binary) linear classifier is a classifier that separates two classes using a line, a plane, or a hyperplane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n5o8QeEwc59m"
   },
   "source": [
    "The two most common linear classification algorithms are logistic regression, implemented in linear_model.LogisticRegression, and linear support vector machines (linear SVMs), implemented in svm.LinearSVC (SVC stands for support vector classi‐ fier). Despite its name, LogisticRegression is a classification algorithm and not a regression algorithm, and it should not be confused with LinearRegression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XyRV64SEjT5P"
   },
   "source": [
    "**Logistic Regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rWF2u82Ujk0f"
   },
   "source": [
    "**Step 1 - Loading the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "4zbkB30Tjk0g",
    "outputId": "5d41e8ec-ff93-4058-81fe-b2f43a94d87c"
   },
   "outputs": [],
   "source": [
    "X, y = mglearn.datasets.make_forge()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1mEQ1UMbjk0k"
   },
   "source": [
    "**Step 2 - Train-Test Split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FEm7tzBNjk0l"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-ocqp7NQjk0n"
   },
   "source": [
    "**Step 3 - Building the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZI0Nq8qejk0o"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QW2N7fdCjk0q"
   },
   "source": [
    "**Step 4 - Training the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "T4Bo95djjk0q",
    "outputId": "9551f368-2539-48af-af5e-9ad809d769b8"
   },
   "outputs": [],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zF1IUOscjk0t"
   },
   "source": [
    "**Step 5 - Making the predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ASuWQXTtjk0u",
    "outputId": "5df73e53-e4e0-408a-d346-12afa1133293"
   },
   "outputs": [],
   "source": [
    "print(\"Test set prediction {}\".format(clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "toAgjbpIjk0w"
   },
   "source": [
    "**Step 6 - Evaluating the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "eQoqHpkdjk0w",
    "outputId": "6a9a2598-d9d4-4e20-a3cb-31420c8c0090"
   },
   "outputs": [],
   "source": [
    "print(\"Train set accuracy : {}\".format(clf.score(X_train, y_train)))\n",
    "print(\"Test set accuracy : {}\".format(clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "00iKU4gdkCJG"
   },
   "source": [
    "**Linear Support Vector Classifier**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "beMTbhCukKlj"
   },
   "source": [
    "**Step 1 - Loading the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "fYNIA6ENkKlk",
    "outputId": "8c3ddc43-e76d-4b88-a7f1-0dbd380f4ea7"
   },
   "outputs": [],
   "source": [
    "X, y = mglearn.datasets.make_forge()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gsDwR7KqkKlo"
   },
   "source": [
    "**Step 2 - Train-Test Split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gOSt5WMdkKlo"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u7fH8LmbkKls"
   },
   "source": [
    "**Step 3 - Building the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "viY5LpuBkKls"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "clf = LinearSVC()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1VEjEe90kKlu"
   },
   "source": [
    "**Step 4 - Training the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "XL4XqAKNkKlv",
    "outputId": "6a758ace-4e34-402d-cb0d-728fdb31ffb4"
   },
   "outputs": [],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Asu-kr42kKlw"
   },
   "source": [
    "**Step 5 - Making the predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "cb0MJ9uukKlx",
    "outputId": "80d59f53-8b24-4bb0-a73f-508cde31e8a3"
   },
   "outputs": [],
   "source": [
    "print(\"Test set prediction {}\".format(clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-ueBE1JZkKlz"
   },
   "source": [
    "**Step 6 - Evaluating the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "mZPDv4ggkKlz",
    "outputId": "aaa0f698-b3b9-4c3e-a83f-1f6ade71db01"
   },
   "outputs": [],
   "source": [
    "print(\"Train set accuracy : {}\".format(clf.score(X_train, y_train)))\n",
    "print(\"Test set accuracy : {}\".format(clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "faOTe5bMkuA3"
   },
   "source": [
    "For LogisticRegression and LinearSVC the trade-off parameter that determines the strength of the regularization is called C, and higher values of C correspond to less regularization. In other words, when you use a high value for the parameter C, Logis ticRegression and LinearSVC try to fit the training set as best as possible, while with low values of the parameter C, the models put more emphasis on finding a coefficient vector (w) that is close to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gkqwGjfqlKfk"
   },
   "source": [
    "There is another interesting aspect of how the parameter C acts. Using low values of C will cause the algorithms to try to adjust to the “majority” of data points, while using a higher value of C stresses the importance that each individual data point be classi‐ fied correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 268
    },
    "colab_type": "code",
    "id": "KomQ_ItFlLsx",
    "outputId": "98da9b10-fd00-414a-8718-b4f58d99dd44"
   },
   "outputs": [],
   "source": [
    "mglearn.plots.plot_linear_svc_regularization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Yp7Ntdhhl0em"
   },
   "source": [
    "LogisticRegression applies an L2 regularization by default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EWdSkx6YmP5H"
   },
   "source": [
    "**Linear models for multiclass classification**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M7Ph7Y2AmUIV"
   },
   "source": [
    "Many linear classification models are for binary classification only, and don’t extend naturally to the multiclass case (with the exception of logistic regression). A common technique to extend a binary classification algorithm to a multiclass classification algorithm is the one-vs.-rest approach. In the one-vs.-rest approach, a binary model is learned for each class that tries to separate that class from all of the other classes, resulting in as many binary models as there are classes. To make a prediction, all binary classifiers are run on a test point. The classifier that has the highest score on its single class “wins,” and this class label is returned as the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 313
    },
    "colab_type": "code",
    "id": "JiVtkjYNmeEM",
    "outputId": "ab6d0502-1d97-4cb8-afce-2849e7a5c0d8"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "X, y = make_blobs(random_state=42) \n",
    "mglearn.discrete_scatter(X[:, 0], X[:, 1], y) \n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\")\n",
    "plt.legend([\"Class 0\", \"Class 1\", \"Class 2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "7fKZX1zDmxNY",
    "outputId": "dad892aa-232c-4f4a-883c-66dbc283d70f"
   },
   "outputs": [],
   "source": [
    "linear_svm = LinearSVC().fit(X, y)\n",
    "print(\"Coefficient shape: \", linear_svm.coef_.shape) \n",
    "print(\"Intercept shape: \", linear_svm.intercept_.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8bXlLtkEm6Am"
   },
   "source": [
    "We see that the shape of the coef_ is (3, 2), meaning that each row of coef_ contains the coefficient vector for one of the three classes and each column holds the coefficient value for a specific feature (there are two in this dataset). The intercept_ is now a one-dimensional array, storing the intercepts for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "colab_type": "code",
    "id": "VoZcoNUom8gD",
    "outputId": "d8b293cf-4efb-470a-8ecd-7399998dc177"
   },
   "outputs": [],
   "source": [
    "mglearn.discrete_scatter(X[:, 0], X[:, 1], y)\n",
    "line = np.linspace(-15, 15)\n",
    "for coef, intercept, color in zip(linear_svm.coef_, linear_svm.intercept_,mglearn.cm3.colors): \n",
    "  plt.plot(line, -(line * coef[0] + intercept) / coef[1], c=color)\n",
    "plt.ylim(-10, 15)\n",
    "plt.xlim(-10, 8)\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\")\n",
    "plt.legend(['Class 0', 'Class 1', 'Class 2', 'Line class 0', 'Line class 1','Line class 2'], loc=(1.01, 0.3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6_evJhgunPFI"
   },
   "source": [
    "You can see that all the points belonging to class 0 in the training data are above the line corresponding to class 0, which means they are on the “class 0” side of this binary classifier. The points in class 0 are above the line corresponding to class 2, which means they are classified as “rest” by the binary classifier for class 2. The points belonging to class 0 are to the left of the line corresponding to class 1, which means the binary classifier for class 1 also classifies them as “rest.” Therefore, any point in this area will be classified as class 0 by the final classifier (the result of the classification confidence formula for classifier 0 is greater than zero, while it is smaller than zero for the other two classes).\n",
    "\n",
    "But what about the triangle in the middle of the plot? All three binary classifiers classify points there as “rest.” Which class would a point there be assigned to? The answer is the one with the highest value for the classification formula: the class of the closest line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "colab_type": "code",
    "id": "RIdEKZE2neDo",
    "outputId": "ce17bdd2-4bec-45f6-e97f-37d7c7246cb1"
   },
   "outputs": [],
   "source": [
    "mglearn.plots.plot_2d_classification(linear_svm, X, fill=True, alpha=.7) \n",
    "mglearn.discrete_scatter(X[:, 0], X[:, 1], y)\n",
    "line = np.linspace(-15, 15)\n",
    "for coef, intercept, color in zip(linear_svm.coef_, linear_svm.intercept_,mglearn.cm3.colors): \n",
    "  plt.plot(line, -(line * coef[0] + intercept) / coef[1], c=color)\n",
    "plt.legend(['Class 0', 'Class 1', 'Class 2', 'Line class 0', 'Line class 1', 'Line class 2'], loc=(1.01, 0.3))\n",
    "plt.xlabel(\"Feature 0\") \n",
    "plt.ylabel(\"Feature 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UH9hn9Nxno16"
   },
   "source": [
    "**Strengths, weakness and parameters Linear models(Classifier and Regressor)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gNT0MBGwnwuU"
   },
   "source": [
    "The main parameter of linear models is the regularization parameter, called alpha in the regression models and C in LinearSVC and LogisticRegression. Large values for alpha or small values for C mean simple models. In particular for the regression mod‐ els, tuning these parameters is quite important. Usually C and alpha are searched for on a logarithmic scale. The other decision you have to make is whether you want to use L1 regularization or L2 regularization. If you assume that only a few of your fea‐ tures are actually important, you should use L1. Otherwise, you should default to L2. L1 can also be useful if interpretability of the model is important. As L1 will use only a few features, it is easier to explain which features are important to the model, and what the effects of these features are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9zjuMuqen5mt"
   },
   "source": [
    "Linear models are very fast to train, and also fast to predict. They scale to very large datasets and work well with sparse data. If your data consists of hundreds of thou‐ sands or millions of samples, you might want to investigate using the solver='sag' option in LogisticRegression and Ridge, which can be faster than the default on large datasets. Other options are the SGDClassifier class and the SGDRegressor class, which implement even more scalable versions of the linear models described here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JtEu3DZ5oBsS"
   },
   "source": [
    "Linear models often perform well when the number of features is large compared to the number of samples. They are also often used on very large datasets, simply because it’s not feasible to train other models. However, in lowerdimensional spaces, other models might yield better generalization performance."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyObRW+Gvs/qbfYOMBpM4S5N",
   "include_colab_link": true,
   "name": "4.2 - Supervised Learning - Linear Models.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
